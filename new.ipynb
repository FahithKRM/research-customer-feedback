{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25aa7c3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SECTION 1: SETUP AND INSTALLATIONS\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "from google.colab import drive\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "print(\"✅ All libraries imported.\")\n",
    "\n",
    "# Download necessary NLTK data (only needs to be run once)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4') # Open Multilingual Wordnet\n",
    "print(\"✅ NLTK components downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913371cc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SECTION 2: DATA LOADING AND PREPARATION\n",
    "# ==============================================================================\n",
    "# To access files from Google Drive, you must first mount it.\n",
    "# This will open an authentication pop-up. Follow the steps to grant access.\n",
    "print(\"Mounting Google Drive...\")\n",
    "drive.mount('/content/drive')\n",
    "print(\"Google Drive mounted successfully.\")\n",
    "\n",
    "# Define the full path to your CSV file on Google Drive.\n",
    "# The path starts with '/content/drive/My Drive/'\n",
    "# and then follows the folder structure you provided.\n",
    "DATA_PATH = '/content/drive/My Drive/Dataset/consumer-complaints.csv'\n",
    "\n",
    "# Load the dataset from the specified Google Drive path.\n",
    "# We'll specifically select the 'product' and 'consumer_complaint_narrative' columns\n",
    "# as they are the most relevant for our classification task. We also drop any rows\n",
    "# that have missing values in these columns to ensure data quality.\n",
    "try:\n",
    "    df = pd.read_csv(DATA_PATH)[['product', 'consumer_complaint_narrative']].dropna()\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file at '{DATA_PATH}' was not found. Please check the path and try again.\")\n",
    "    # Exit or handle the error gracefully if the file isn't found.\n",
    "    # For this example, we'll just stop here.\n",
    "    exit()\n",
    "\n",
    "# We'll select a subset of the data that focuses on specific product types.\n",
    "# For this task, we're interested in 'Debt collection' and 'Mortgage' complaints.\n",
    "# These will form our two classes for binary classification.\n",
    "df_useful = df[df['product'].isin(['Debt collection', 'Mortgage'])]\n",
    "\n",
    "# To make the processing faster and manageable, we will take a smaller,\n",
    "# random sample of 10,000 records from this filtered dataset.\n",
    "# The 'random_state' is set to 42 to ensure the sample is the same every time\n",
    "# the script is run, which is crucial for reproducibility.\n",
    "df_useful = df_useful.sample(n=10000, random_state=42)\n",
    "\n",
    "# Now, we create our target variable, or 'sentiment'.\n",
    "# We'll assign a label of 1 to 'Debt collection' complaints (representing one class)\n",
    "# and a label of 0 to 'Mortgage' complaints (representing the other class).\n",
    "df_useful['sentiment'] = np.where(df_useful['product'] == 'Debt collection', 1, 0)\n",
    "\n",
    "# Finally, we'll keep only the text and the newly created sentiment columns.\n",
    "# We also rename the 'consumer_complaint_narrative' column to 'text' for\n",
    "# convenience and to align with common practices in text processing pipelines.\n",
    "df_useful.rename(columns={'consumer_complaint_narrative': 'text'}, inplace=True)\n",
    "df_useful = df_useful[['text', 'sentiment']]\n",
    "\n",
    "# Let's print the first few rows of our new, useful dataset to confirm the\n",
    "# structure and the newly added 'sentiment' column.\n",
    "print(\"\\n--- Processed Dataset Head ---\")\n",
    "print(df_useful.head())\n",
    "\n",
    "# It's also good practice to check the distribution of our target variable.\n",
    "# This helps us understand if the classes are balanced or if we might need\n",
    "# to apply techniques to handle class imbalance.\n",
    "print(\"\\n--- Data Distribution ---\")\n",
    "print(df_useful['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ff088c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SECTION 3: TEXT PREPROCESSING\n",
    "# ==============================================================================\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and prepares the text for feature extraction.\n",
    "    \"\"\"\n",
    "    # 1. Remove HTML tags (if any)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # 2. Remove non-alphabetic characters and convert to lower case\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
    "    # 3. Tokenize\n",
    "    tokens = text.split()\n",
    "    # 4. Remove stopwords and lemmatize\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 2]\n",
    "    return \" \".join(lemmatized_tokens)\n",
    "\n",
    "print(\"\\nPreprocessing data... (This may take a minute)\")\n",
    "df_sample['processed_text'] = df_sample['text'].apply(preprocess_text)\n",
    "print(\"✅ Preprocessing complete.\")\n",
    "\n",
    "print(\"\\n--- Original vs. Processed Text ---\")\n",
    "print(\"Original:\", df_sample['text'].iloc[0])\n",
    "print(\"Processed:\", df_sample['processed_text'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee8e4da",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SECTION 4: RULE-BASED COMPONENT\n",
    "# ==============================================================================\n",
    "# List of strong negative keywords and phrases indicating a complaint/issue\n",
    "NEGATIVE_KEYWORDS = [\n",
    "    'fraud', 'incorrect', 'scam', 'unauthorized', 'stolen', 'never received',\n",
    "    'not resolved', 'false information', 'dispute', 'complaint', 'inaccurate',\n",
    "    'wrong', 'thief', 'damage', 'violation'\n",
    "]\n",
    "\n",
    "def predict_with_rules(text):\n",
    "    \"\"\"\n",
    "    Applies a set of predefined rules to classify text as negative.\n",
    "    Returns 1 (Negative) if a rule is triggered, otherwise -1 (Unknown).\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    if any(keyword in text for keyword in NEGATIVE_KEYWORDS):\n",
    "        return 1\n",
    "    if \"not accurate\" in text or \"not correct\" in text:\n",
    "        return 1\n",
    "    return -1 # Use -1 to signify 'unknown'\n",
    "\n",
    "print(\"\\n✅ Rule-based component defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3054a0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SECTION 5: FEATURE EXTRACTION & DATA SPLITTING\n",
    "# ==============================================================================\n",
    "# Define features (X) and target (y)\n",
    "X_raw = df_sample['text'] # For rule-based model\n",
    "X_processed = df_sample['processed_text'] # For ML models\n",
    "y = df_sample['sentiment']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_raw, X_test_raw, X_train_processed, X_test_processed, y_train, y_test = train_test_split(\n",
    "    X_raw, X_processed, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"\\nTraining data shape: {X_train_processed.shape}\")\n",
    "print(f\"Testing data shape: {X_test_processed.shape}\")\n",
    "\n",
    "# Initialize and fit the TF-IDF Vectorizer on the processed training data\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_processed)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_processed)\n",
    "\n",
    "print(\"✅ TF-IDF vectorization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd634e59",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SECTION 6: TRAINING MACHINE LEARNING MODELS\n",
    "# ==============================================================================\n",
    "print(\"\\nTraining ML models...\")\n",
    "\n",
    "# 1. Logistic Regression\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_model.fit(X_train_tfidf, y_train)\n",
    "print(\"✅ Logistic Regression trained.\")\n",
    "\n",
    "# 2. Multinomial Naive Bayes\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "print(\"✅ Naive Bayes trained.\")\n",
    "\n",
    "# 3. Linear SVM (Support Vector Machine)\n",
    "svm_model = LinearSVC(random_state=42)\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "print(\"✅ Linear SVM trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9d9c53",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SECTION 7: IMPLEMENTING AND TESTING THE HYBRID MODEL\n",
    "# ==============================================================================\n",
    "print(\"\\nImplementing the hybrid model logic...\")\n",
    "\n",
    "def predict_hybrid(raw_texts, tfidf_vectors, ml_model):\n",
    "    \"\"\"\n",
    "    Combines rule-based predictions with an ML model.\n",
    "    \"\"\"\n",
    "    hybrid_predictions = []\n",
    "    # Convert raw_texts Series to a list to use index\n",
    "    raw_texts_list = list(raw_texts)\n",
    "\n",
    "    for i, text in enumerate(raw_texts_list):\n",
    "        # 1. Apply rule-based system on raw text\n",
    "        rule_pred = predict_with_rules(text)\n",
    "        if rule_pred != -1: # Rule triggered\n",
    "            hybrid_predictions.append(rule_pred)\n",
    "        else:\n",
    "            # 2. If no rule, use the ML model on the corresponding TF-IDF vector\n",
    "            vector = tfidf_vectors[i]\n",
    "            ml_pred = ml_model.predict(vector)[0]\n",
    "            hybrid_predictions.append(ml_pred)\n",
    "    return np.array(hybrid_predictions)\n",
    "\n",
    "# Generate predictions for all models\n",
    "lr_preds = lr_model.predict(X_test_tfidf)\n",
    "nb_preds = nb_model.predict(X_test_tfidf)\n",
    "svm_preds = svm_model.predict(X_test_tfidf)\n",
    "hybrid_preds = predict_hybrid(X_test_raw, X_test_tfidf, svm_model) # Using SVM as the ML component\n",
    "\n",
    "print(\"✅ All model predictions are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c725158",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SECTION 8: EVALUATION AND COMPARISON\n",
    "# ==============================================================================\n",
    "target_names = ['Class 0 (Positive)', 'Class 1 (Negative)']\n",
    "\n",
    "print(\"\\n\\n--- MODEL PERFORMANCE COMPARISON ---\")\n",
    "\n",
    "print(\"\\n--- 1. Logistic Regression ---\")\n",
    "print(classification_report(y_test, lr_preds, target_names=target_names))\n",
    "\n",
    "print(\"\\n--- 2. Naive Bayes ---\")\n",
    "print(classification_report(y_test, nb_preds, target_names=target_names))\n",
    "\n",
    "print(\"\\n--- 3. SVM (Standalone) ---\")\n",
    "print(classification_report(y_test, svm_preds, target_names=target_names))\n",
    "\n",
    "print(\"\\n--- 4. HYBRID MODEL (Rules + SVM) --- 🏆\")\n",
    "print(classification_report(y_test, hybrid_preds, target_names=target_names))\n",
    "\n",
    "print(\"\\n--- Summary of Accuracy Scores ---\")\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_score(y_test, lr_preds):.4f}\")\n",
    "print(f\"Naive Bayes Accuracy:         {accuracy_score(y_test, nb_preds):.4f}\")\n",
    "print(f\"SVM Accuracy:                 {accuracy_score(y_test, svm_preds):.4f}\")\n",
    "print(f\"Hybrid Model Accuracy:        {accuracy_score(y_test, hybrid_preds):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce711716",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SECTION 9: PREDICTION ON NEW, UNSEEN DATA\n",
    "# ==============================================================================\n",
    "\n",
    "# Ensure all your models (lr_model, nb_model, svm_model) and the\n",
    "# tfidf_vectorizer are already trained and available in the environment.\n",
    "\n",
    "def predict_new_review(text):\n",
    "    \"\"\"\n",
    "    Takes a new text string and returns predictions from all trained models.\n",
    "    \"\"\"\n",
    "    print(f\"--- Analyzing Review: '{text}' ---\")\n",
    "\n",
    "    # 1. Preprocess the new text using the same function\n",
    "    processed_text = preprocess_text(text)\n",
    "    \n",
    "    # 2. Vectorize the processed text using the FITTED TF-IDF vectorizer\n",
    "    # IMPORTANT: Use .transform() only, NOT .fit_transform()\n",
    "    vectorized_text = tfidf_vectorizer.transform([processed_text])\n",
    "\n",
    "    # 3. Get predictions from the standard ML models\n",
    "    lr_pred = lr_model.predict(vectorized_text)[0]\n",
    "    nb_pred = nb_model.predict(vectorized_text)[0]\n",
    "    svm_pred = svm_model.predict(vectorized_text)[0]\n",
    "    \n",
    "    # 4. Get prediction from the Hybrid Model\n",
    "    # The hybrid model's logic checks rules on the original raw text first\n",
    "    hybrid_pred = -1\n",
    "    rule_result = predict_with_rules(text) # Check rules on original text\n",
    "    if rule_result != -1:\n",
    "        hybrid_pred = rule_result\n",
    "    else:\n",
    "        # If no rule triggered, use the SVM prediction we already made\n",
    "        hybrid_pred = svm_pred\n",
    "\n",
    "    # 5. Decode predictions for readability\n",
    "    # 1 is 'Negative' and 0 is 'Positive' in our setup\n",
    "    sentiment_map = {1: 'Negative 😡', 0: 'Positive ✅'}\n",
    "    \n",
    "    print(f\"Logistic Regression Prediction: {sentiment_map[lr_pred]}\")\n",
    "    print(f\"Naive Bayes Prediction:         {sentiment_map[nb_pred]}\")\n",
    "    print(f\"SVM Prediction:                 {sentiment_map[svm_pred]}\")\n",
    "    print(f\"Hybrid Model Prediction:      {sentiment_map[hybrid_pred]}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# NOW, LET'S TEST IT WITH YOUR OWN SENTENCES!\n",
    "# ==============================================================================\n",
    "\n",
    "# Example 1: A clearly negative complaint\n",
    "new_complaint_1 = \"There is an incorrect and fraudulent charge on my credit report from a company I have never heard of. This is damaging my score.\"\n",
    "predict_new_review(new_complaint_1)\n",
    "\n",
    "# Example 2: A text that should be classified as positive in our context (about a mortgage)\n",
    "new_complaint_2 = \"My application for the mortgage went through smoothly and the agent was very helpful.\"\n",
    "predict_new_review(new_complaint_2)\n",
    "\n",
    "# Example 3: A tricky case without strong negative keywords for the rule-based system\n",
    "new_complaint_3 = \"The information listed on my account is not up to date and it's causing a lot of problems with my financial planning.\"\n",
    "predict_new_review(new_complaint_3)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

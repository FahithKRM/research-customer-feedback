{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c220e0a3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Full Code for Automated Customer Feedback Analysis Research\n",
    "# Hybrid Approach: Rule-Based + Machine Learning (Logistic Regression with TF-IDF)\n",
    "# Focus: Detect Negative Product Feedback Impacts\n",
    "# Environment: Google Colab\n",
    "# Dataset: Loaded from Google Drive (dummy_customer_feedback.csv or replace with real dataset)\n",
    "# Columns Used: 'review_text' (input), 'sentiment' (label: Positive, Negative, Neutral)\n",
    "# Comparisons: Unigram, Bigram, Trigram features; NB, LR classifiers; Hybrid adjustments\n",
    "# Outputs: Confusion Matrices, Evaluation Measures (P, R, F), Bar Charts, Accuracy vs. Sample Size\n",
    "\n",
    "# Step 1: Install Dependencies\n",
    "!pip install nltk scikit-learn matplotlib seaborn pandas numpy\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Step 2: Mount Google Drive and Load Dataset\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Update file_path to your dataset location in Google Drive\n",
    "file_path = '/content/drive/MyDrive/ResearchData/dummy_customer_feedback.csv'  # Replace with your path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Step 3: Select Relevant Columns and Preprocess\n",
    "# Select columns: 'review_text' for input, 'sentiment' for labels (remove IDs, etc.)\n",
    "df_selected = df[['review_text', 'sentiment']]\n",
    "\n",
    "# Clean text: Lowercase, remove punctuation\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "df_selected['review_text'] = df_selected['review_text'].apply(clean_text)\n",
    "\n",
    "# Encode labels: Positive=2, Neutral=1, Negative=0\n",
    "label_encoder = LabelEncoder()\n",
    "df_selected['sentiment_encoded'] = label_encoder.fit_transform(df_selected['sentiment'])\n",
    "\n",
    "# Check distribution\n",
    "print(df_selected['sentiment'].value_counts())\n",
    "\n",
    "# Step 4: Rule-Based Component\n",
    "# Simple rule-based sentiment: Detect negations and negative keywords for hybrid adjustment\n",
    "negative_keywords = ['bad', 'poor', 'terrible', 'disappointing', 'frustrating', 'broke', 'crash', 'slow', 'overheat', 'drain', 'weak', 'not good', 'not worth']\n",
    "negation_words = ['not', 'no', 'never', 'none', 'nothing']\n",
    "\n",
    "def rule_based_sentiment(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    score = 0  # Positive: >0, Negative: <0, Neutral: 0\n",
    "    negation = False\n",
    "    for token in tokens:\n",
    "        if token in negation_words:\n",
    "            negation = True\n",
    "            continue\n",
    "        if token in negative_keywords:\n",
    "            score -= 2 if negation else -1\n",
    "        negation = False  # Reset after word\n",
    "    if score < 0:\n",
    "        return 0  # Negative\n",
    "    elif score > 0:\n",
    "        return 2  # Positive\n",
    "    else:\n",
    "        return 1  # Neutral\n",
    "\n",
    "# Apply rule-based as a feature or for hybrid\n",
    "df_selected['rule_sentiment'] = df_selected['review_text'].apply(rule_based_sentiment)\n",
    "\n",
    "# Step 5: ML Component and Hybrid Model\n",
    "# Function to train and evaluate models with different n-grams\n",
    "def evaluate_model(ngram_range, classifier, model_name, sample_sizes=[10000, 15000, 25000]):\n",
    "    results = {}\n",
    "    accuracies = []\n",
    "    for size in sample_sizes:\n",
    "        if size > len(df_selected):\n",
    "            size = len(df_selected)  # Adjust if dataset is small\n",
    "        df_sample = df_selected.sample(n=size, random_state=42)\n",
    "        \n",
    "        X = df_sample['review_text']\n",
    "        y = df_sample['sentiment_encoded']\n",
    "        \n",
    "        # TF-IDF Vectorizer\n",
    "        vectorizer = TfidfVectorizer(ngram_range=ngram_range, max_features=5000)\n",
    "        X_vec = vectorizer.fit_transform(X)\n",
    "        \n",
    "        # Train-Test Split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Train Classifier\n",
    "        classifier.fit(X_train, y_train)\n",
    "        y_pred_ml = classifier.predict(X_test)\n",
    "        \n",
    "        # Hybrid: Adjust ML predictions with rule-based (e.g., if rule detects negative impact, override to negative)\n",
    "        y_pred_hybrid = []\n",
    "        for idx, pred in enumerate(y_pred_ml):\n",
    "            text = df_sample['review_text'].iloc[idx + len(df_sample) - len(y_test)]  # Align index\n",
    "            rule_pred = rule_based_sentiment(text)\n",
    "            if rule_pred == 0:  # If rule detects negative, override\n",
    "                y_pred_hybrid.append(0)\n",
    "            else:\n",
    "                y_pred_hybrid.append(pred)\n",
    "        \n",
    "        # Evaluation\n",
    "        cm = confusion_matrix(y_test, y_pred_hybrid)\n",
    "        report = classification_report(y_test, y_pred_hybrid, output_dict=True, target_names=label_encoder.classes_)\n",
    "        acc = accuracy_score(y_test, y_pred_hybrid)\n",
    "        accuracies.append(acc)\n",
    "        \n",
    "        # Store for n-gram\n",
    "        results[f'{model_name} - Size {size}'] = {'cm': cm, 'report': report, 'acc': acc}\n",
    "    \n",
    "    return results, accuracies, sample_sizes\n",
    "\n",
    "# Models\n",
    "classifiers = {\n",
    "    'NB': MultinomialNB(),\n",
    "    'LR': LogisticRegression(max_iter=1000)\n",
    "}\n",
    "\n",
    "# N-gram ranges\n",
    "ngrams = {\n",
    "    'Unigram': (1,1),\n",
    "    'Bigram': (2,2),\n",
    "    'Trigram': (3,3)\n",
    "}\n",
    "\n",
    "# Run evaluations\n",
    "all_results = {}\n",
    "all_accuracies = {'NB': [], 'LR': []}\n",
    "sample_sizes_adjusted = [1000, 800, 600]  # Adjust for small dummy dataset; change for larger\n",
    "\n",
    "for ngram_name, ngram_range in ngrams.items():\n",
    "    for model_name, clf in classifiers.items():\n",
    "        results, accs, _ = evaluate_model(ngram_range, clf, f'{ngram_name}-{model_name}', sample_sizes=sample_sizes_adjusted)\n",
    "        all_results.update(results)\n",
    "        if model_name == 'NB':\n",
    "            all_accuracies['NB'].extend(accs)\n",
    "        else:\n",
    "            all_accuracies['LR'].extend(accs)\n",
    "\n",
    "# Step 6: Generate Outputs like Images\n",
    "# 6.1: Confusion Matrices and Evaluation Measures Tables\n",
    "for key, res in all_results.items():\n",
    "    print(f\"\\nN-gram Feature: {key}\")\n",
    "    cm = res['cm']\n",
    "    report = res['report']\n",
    "    \n",
    "    # Confusion Matrix (assuming binary for simplicity; adjust for multi-class)\n",
    "    # Note: Images show binary (Positive/Negative), but dataset has 3 classes. Simplify to binary for match.\n",
    "    # Map Neutral to Positive for binary approximation\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(pd.DataFrame(cm, index=label_encoder.classes_, columns=label_encoder.classes_))\n",
    "    \n",
    "    # Evaluation Measures Table\n",
    "    measures = {\n",
    "        'P': [report[label]['precision']*100 for label in label_encoder.classes_],\n",
    "        'R': [report[label]['recall']*100 for label in label_encoder.classes_],\n",
    "        'F': [report[label]['f1-score']*100 for label in label_encoder.classes_]\n",
    "    }\n",
    "    print(\"Evaluation Measures:\")\n",
    "    print(pd.DataFrame(measures, index=label_encoder.classes_))\n",
    "\n",
    "# 6.2: Bar Chart for Evaluation Measures (Average across classes)\n",
    "avg_measures = {\n",
    "    'Precision': np.mean([report['weighted avg']['precision']*100 for res in all_results.values()]),\n",
    "    'Recall': np.mean([report['weighted avg']['recall']*100 for res in all_results.values()]),\n",
    "    'F-Measure': np.mean([report['weighted avg']['f1-score']*100 for res in all_results.values()])\n",
    "}\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(avg_measures.keys(), avg_measures.values(), color=['blue', 'orange', 'gray'])\n",
    "ax.set_title('Evaluation Measures')\n",
    "ax.set_ylabel('Percentage (%)')\n",
    "plt.show()\n",
    "\n",
    "# 6.3: Classification Accuracy Bar Chart vs. No. of Reviews\n",
    "fig, ax = plt.subplots()\n",
    "width = 0.25\n",
    "x = np.arange(len(sample_sizes_adjusted))\n",
    "ax.bar(x - width/2, all_accuracies['NB'], width, label='NB', color='orange')\n",
    "ax.bar(x + width/2, all_accuracies['LR'], width, label='LR', color='gray')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(sample_sizes_adjusted)\n",
    "ax.set_title('Classification Accuracy')\n",
    "ax.set_xlabel('No. of Reviews')\n",
    "ax.set_ylabel('Accuracy (%)')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

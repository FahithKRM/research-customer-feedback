# Step 1: Set Up Environment and Install Dependencies
!pip install -q pandas numpy scikit-learn nltk textblob spacy transformers sentence-transformers
!python -m spacy download en_core_web_sm

import nltk
nltk.download('vader_lexicon')
nltk.download('punkt')
nltk.download('stopwords')

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from textblob import TextBlob
import spacy
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
from scipy.sparse import hstack

nlp = spacy.load('en_core_web_sm')
sia = SentimentIntensityAnalyzer()

print("Environment setup complete!")

# Step 2: Mount Google Drive and Load Dataset
from google.colab import drive
drive.mount('/content/drive')

# Load dataset (adjust path if needed)
data_path = '/content/drive/MyDrive/Dataset/movie_review.csv'  # Update to your path
df = pd.read_csv(data_path)

print("Dataset loaded:")
print(df.head())
print(f"Shape: {df.shape}")
print("Sentiment distribution:", df['tag'].value_counts())

# Step 3: Select Relevant Columns and Preprocess
# Keep only 'Issue' and 'Sub-issue'
df = df[['text', 'tag']]

# Fill NaN in 'Sub-issue' with empty string to avoid errors
df['text'] = df['text'].fillna("")
df['tag'] = df['tag'].fillna("")

# Example: If you want to classify based on 'Issue' only
# Use 'Issue' as label
le = LabelEncoder()
df['issue_encoded'] = le.fit_transform(df['tag'])
print("Encoded classes:", le.classes_)

# Train-test split (80/20)
X = df['text']
y = df['issue_encoded']
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print("Data prepared. Train shape:", X_train.shape, "Test shape:", X_test.shape)
print("Train set issue distribution:\n", pd.Series(y_train).value_counts())
print("Test set issue distribution:\n", pd.Series(y_test).value_counts())

# Step 4: Rule-Based Model with Enhanced Negation Handling
def rule_based_sentiment(text):
    # VADER sentiment
    vader_score = sia.polarity_scores(text)['compound']

    # Enhance with spaCy negation detection
    doc = nlp(text)
    negation = any(token.dep_ == 'neg' for token in doc)

    # Adjust thresholds and handle negations
    if negation and vader_score > 0:
        return 0  # Flip to Negative if negation detected
    elif vader_score > 0.1:  # Tighter threshold for Positive
        return 1  # Positive (changed from 2)
    elif vader_score < -0.1:  # Tighter threshold for Negative
        return 0  # Negative
    else:
        # Since there's no 'Neutral' in this dataset, we'll assign close scores
        # to the closest sentiment or re-evaluate. For now, assign to the dominant class if no strong sentiment.
        # This part might need adjustment based on how 'neutral' should be handled in a binary classification context.
        # Given the dataset only has 'Negative' and 'Positive', we'll force a binary output.
        return 1 if vader_score >= 0 else 0


# Apply to test set
y_pred_rule = [rule_based_sentiment(text) for text in X_test]

# Evaluate Rule-Based
print("Rule-Based Evaluation:")
# Ensure target_names matches the actual classes present
print(classification_report(y_test, y_pred_rule, target_names=le.classes_, zero_division=0))
acc_rule = accuracy_score(y_test, y_pred_rule)

# Step 5 & 6: ML Models with N-gram Features (TF-IDF with 1-3 grams), Evaluation
# Vectorizer for n-grams (reduced features to prevent overfitting)
vectorizer = TfidfVectorizer(ngram_range=(1, 3), stop_words='english', max_features=2000)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Model 1: Logistic Regression (with regularization)
lr_model = LogisticRegression(max_iter=1000, C=0.5)
lr_model.fit(X_train_tfidf, y_train)
y_pred_lr = lr_model.predict(X_test_tfidf)
acc_lr = accuracy_score(y_test, y_pred_lr)
print("Logistic Regression Evaluation:")
print(classification_report(y_test, y_pred_lr, target_names=le.classes_, zero_division=0))

# Model 2: SVM
svm_model = SVC(kernel='linear', C=0.5)
svm_model.fit(X_train_tfidf, y_train)
y_pred_svm = svm_model.predict(X_test_tfidf)
acc_svm = accuracy_score(y_test, y_pred_svm)
print("SVM Evaluation:")
print(classification_report(y_test, y_pred_svm, target_names=le.classes_, zero_division=0))

# Model 3: Random Forest
rf_model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)
rf_model.fit(X_train_tfidf, y_train)
y_pred_rf = rf_model.predict(X_test_tfidf)
acc_rf = accuracy_score(y_test, y_pred_rf)
print("Random Forest Evaluation:")
print(classification_report(y_test, y_pred_rf, target_names=le.classes_, zero_division=0))

# Step 7: Hybrid Approach (Combine Rule-Based Scores with ML Features)
rule_scores_train = np.array([sia.polarity_scores(text)['compound'] for text in X_train]).reshape(-1, 1)
rule_scores_test = np.array([sia.polarity_scores(text)['compound'] for text in X_test]).reshape(-1, 1)

# Combine TF-IDF with rule-based scores
X_train_hybrid = hstack((X_train_tfidf, rule_scores_train))
X_test_hybrid = hstack((X_test_tfidf, rule_scores_test))

# Train Hybrid on Logistic Regression
hybrid_model = LogisticRegression(max_iter=1000, C=0.5)
hybrid_model.fit(X_train_hybrid, y_train)
y_pred_hybrid = hybrid_model.predict(X_test_hybrid)
acc_hybrid = accuracy_score(y_test, y_pred_hybrid)
print("Hybrid Model Evaluation:")
print(classification_report(y_test, y_pred_hybrid, target_names=le.classes_, zero_division=0))

# Step 8: Compare All Models
results = {
    'Model': ['Rule-Based', 'Logistic Regression', 'SVM', 'Random Forest', 'Hybrid'],
    'Accuracy': [acc_rule, acc_lr, acc_svm, acc_rf, acc_hybrid],
    'Precision (macro)': [precision_score(y_test, y_pred_rule, average='macro', zero_division=0),
                         precision_score(y_test, y_pred_lr, average='macro', zero_division=0),
                         precision_score(y_test, y_pred_svm, average='macro', zero_division=0),
                         precision_score(y_test, y_pred_rf, average='macro', zero_division=0),
                         precision_score(y_test, y_pred_hybrid, average='macro', zero_division=0)],
    'Recall (macro)': [recall_score(y_test, y_pred_rule, average='macro', zero_division=0),
                       recall_score(y_test, y_pred_lr, average='macro', zero_division=0),
                       recall_score(y_test, y_pred_svm, average='macro', zero_division=0),
                       recall_score(y_test, y_pred_rf, average='macro', zero_division=0),
                       recall_score(y_test, y_pred_hybrid, average='macro', zero_division=0)],
    'F1-Score (macro)': [f1_score(y_test, y_pred_rule, average='macro', zero_division=0),
                         f1_score(y_test, y_pred_lr, average='macro', zero_division=0),
                         f1_score(y_test, y_pred_svm, average='macro', zero_division=0),
                         f1_score(y_test, y_pred_rf, average='macro', zero_division=0),
                         f1_score(y_test, y_pred_hybrid, average='macro', zero_division=0)]
}

results_df = pd.DataFrame(results)
print("Model Comparison:")
print(results_df)

# Visualize
plt.figure(figsize=(10, 6))
plt.bar(results_df['Model'], results_df['Accuracy'])
plt.xlabel('Models')
plt.ylabel('Accuracy')
plt.title('Model Performance Comparison')
plt.xticks(rotation=45)
plt.show()

# Step 9: Input Different Reviews and Compare Results
def predict_sentiment(new_review):
    tfidf_vec = vectorizer.transform([new_review])
    rule_score = np.array([sia.polarity_scores(new_review)['compound']]).reshape(-1, 1)
    hybrid_vec = hstack((tfidf_vec, rule_score))

    rule_pred = rule_based_sentiment(new_review)
    lr_pred = lr_model.predict(tfidf_vec)[0]
    svm_pred = svm_model.predict(tfidf_vec)[0]
    rf_pred = rf_model.predict(tfidf_vec)[0]
    hybrid_pred = hybrid_model.predict(hybrid_vec)[0]

    # Decode labels - ensure inverse_transform is called with valid predicted labels
    # The models might predict 0 or 1, which correspond to 'Negative' and 'Positive'
    preds = {
        'Rule-Based': le.inverse_transform([rule_pred])[0],
        'Logistic Regression': le.inverse_transform([lr_pred])[0],
        'SVM': le.inverse_transform([svm_pred])[0],
        'Random Forest': le.inverse_transform([rf_pred])[0],
        'Hybrid': le.inverse_transform([hybrid_pred])[0]
    }
    return preds

# Test new reviews
new_reviews = [
    "The battery life is amazing, lasts all day!",
    "This product is terrible, broke after a week.",
    "It's okay, nothing special but works fine." ,
    "False statements or representation"
]

for review in new_reviews:
    print(f"\nReview: {review}")
    print(predict_sentiment(review))